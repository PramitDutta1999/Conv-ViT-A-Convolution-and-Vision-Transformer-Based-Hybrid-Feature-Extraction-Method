{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2SBkzoOj1SQ"
      },
      "outputs": [],
      "source": [
        "#Google Drive Mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuPMhXywkdm-"
      },
      "outputs": [],
      "source": [
        "#Import Package\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#import tensorflow \n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input,Dense,Reshape,Flatten,Conv2D,Conv2DTranspose,LeakyReLU,ReLU,GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import BatchNormalization,Dropout,Embedding,Activation,Concatenate\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "#from tensorflow.keras.preprocessing.image import image\n",
        "from keras.utils.np_utils import to_categorical \n",
        "from tensorflow.keras import backend as K\n",
        "K.clear_session()\n",
        "#other library \n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from PIL import Image\n",
        "import math\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "#installation\n",
        "!pip install tensorflow-addons==0.8.3\n",
        "#import tensorflow_addons as tfa\n",
        "from mlxtend.plotting import plot_confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhUjbnqdrtoJ"
      },
      "outputs": [],
      "source": [
        "#Dataset\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\" \", 'r') #Dataset File Path\n",
        "zip_ref.extractall(\"/content/\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0jiS26-DHu_"
      },
      "outputs": [],
      "source": [
        "#Data Preprocessing with augmentation\n",
        "\n",
        "TRAINING_DIR = \" \" #Training Set Directory\n",
        "training_datagen = ImageDataGenerator(rescale = 1./255,)\n",
        "                                     \n",
        "VALIDATION_DIR = \" \" #Validation Set Directory\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size=(78,78),\n",
        "    class_mode='categorical',\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    VALIDATION_DIR,\n",
        "    target_size=(78,78),\n",
        "    class_mode='categorical',\n",
        "    batch_size=64\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7yjJkN238Bi"
      },
      "outputs": [],
      "source": [
        "#Model 1\n",
        "input = Input(shape=(84,84,3))\n",
        "pre_trained_model = tf.keras.applications.inception_v3.InceptionV3(input_shape = (84, 84, 3), \n",
        "                                input_tensor = input,\n",
        "                                include_top = False,\n",
        "                                weights = None \n",
        "                                )\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = True\n",
        "inception_layer = pre_trained_model.get_layer('mixed10')\n",
        "inception_layer = inception_layer.output\n",
        "inception_layer = layers.Flatten()(inception_layer)\n",
        "print('last layer output shape: ', inception_layer.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAiwoTOJDZfp"
      },
      "outputs": [],
      "source": [
        "#Model 2\n",
        "pre_trained_model_1 = tf.keras.applications.resnet50.ResNet50(input_shape = (84, 84, 3), \n",
        "                                input_tensor = input,\n",
        "                                include_top = False, \n",
        "                                weights = None \n",
        "                                )\n",
        "for layer in pre_trained_model_1.layers:\n",
        "  layer.trainable = True\n",
        "resnet_layer = pre_trained_model_1.get_layer('conv5_block3_out')\n",
        "#pre_trained_model_1.summary()\n",
        "resnet_layer  = resnet_layer .output\n",
        "#x2 = layers.Conv2D(2048,(4,4),activation='relu')(x2)\n",
        "resnet_layer  = layers.Flatten()(resnet_layer )\n",
        "print('last layer output shape: ', resnet_layer.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_6IxyuJ3rtP"
      },
      "outputs": [],
      "source": [
        "#Vision Transformer\n",
        "#Tuneble Prameters\n",
        "weight_decay = 0.0001\n",
        "num_epochs = 100\n",
        "num_classes=4\n",
        "image_size = 78 \n",
        "patch_size = 6   \n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "] \n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR_CJF-i3r0Y"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_xAa4fs3sfb"
      },
      "outputs": [],
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft_CXORZZI2Z"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD222oTCMIwS"
      },
      "outputs": [],
      "source": [
        "patches = Patches(patch_size)(input)\n",
        "encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "for _ in range(transformer_layers):\n",
        "  x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "  attention_output = layers.MultiHeadAttention(\n",
        "  num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "  )(x1, x1)\n",
        "  x2 = layers.Add()([attention_output, encoded_patches])\n",
        "  x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "  x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "  encoded_patches = layers.Add()([x3, x2])\n",
        "representation = layers.LayerNormalization(name='last_layer',epsilon=1e-6)(encoded_patches)\n",
        "representation = layers.Flatten()(representation)\n",
        "representation = layers.Dropout(0.5)(representation)\n",
        "representation= mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "print('last layer output shape: ', representation.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL0T9-3d4P3U"
      },
      "outputs": [],
      "source": [
        "#Merge Model\n",
        "last_layer = layers.Concatenate()([inception_layer , resnet_layer, representation])\n",
        "x = layers.Dense(1024, activation='relu')(last_layer)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)                  \n",
        "x = layers.BatchNormalization(name='batch_1')(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = ReLU()(x)\n",
        "x = Dense(512)(x)\n",
        "x = layers.BatchNormalization(name='batch_2')(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = ReLU()(x)\n",
        "x = Dense(256)(x)\n",
        "x = BatchNormalization(name='batch_3')(x)\n",
        "x = ReLU()(x)\n",
        "x = layers.Dense  (4, activation='softmax')(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfOntS042YDs"
      },
      "outputs": [],
      "source": [
        "#Callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor= ,\n",
        "                              patience= , min_lr= ,verbose=1) #Tuneable Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kn4dY_YCgSa"
      },
      "outputs": [],
      "source": [
        "#Model Compile \n",
        "model = Model(inputs=input,outputs= x)    \n",
        "model.compile(optimizer = Adam(learning_rate=0.001),\n",
        "              loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False), \n",
        "              metrics = ['accuracy','Precision', 'Recall'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cviyk1V8Xvnh"
      },
      "outputs": [],
      "source": [
        "#Model \n",
        "history = model.fit(\n",
        "            train_generator,\n",
        "            validation_data = validation_generator,\n",
        "            epochs = 20,\n",
        "            verbose = 1,\n",
        "            callbacks=[reduce_lr],\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDoDTZARaykb"
      },
      "outputs": [],
      "source": [
        "#Test Dataset\n",
        "TEST_DIR = \" \" #Test Set Filepath\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "\tTEST_DIR,\n",
        "\ttarget_size=(78,78),\n",
        "\tclass_mode='categorical',\n",
        "\tshuffle=False,\n",
        "  batch_size=64\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYmPe8c4Z8jV"
      },
      "outputs": [],
      "source": [
        "#Model Evaluation\n",
        "results = model.evaluate(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gr2o8_In2tsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ye4AFowt24JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Terminate\n",
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "metadata": {
        "id": "m-6j6FTN24Sa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}